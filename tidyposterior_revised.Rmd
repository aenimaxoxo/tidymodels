---
title: "tidyposterior"
author: "Michael Rose"
output:
  pdf_document:
    highlight:  zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro 

Bayesian analysis is used here to answer the question: "When looking at resampling results, are the differences between models 'real'?" To answer this, a model can be created where the outcome is the resampling statistics (e.g. accuracy or RMSE). These values are explained by the model types. In doing this, we can get parameter estimates for each model's effect on performance and make statistical and practical comparisons between models.

# function summaries 

**perf_mod()** - Bayesian analysis of resampling statistics 

**tidy(perf_mod)** - Extract posterior distributions for models

**contrast_models()** - Estimate the difference between models 

**summary(posterior)** - Summarize the posterior distributions of model statistics 

**summary(posterior_diff)** - Summarize posterior differences of model differences

**ggplot(posterior)** - Visualize the posterior distributions of model statistics 

**ggplot(posterior_diff)** - Visualize the posterior distributions of model differences 

**no_trans, logit_trans, Fisher_trans, ln_trans, inv_trans** - simple transformations

# Index Page 

The tidyposterior package can be used to conduct post hoc analyses of resampling results generated by models.

For example, if two models are evaluated with the root mean squared error (RMSE) using 10-fold cross-validation, there are 10 paired statistics. These can be used to make comparisons between models without involving a test set.

# Example 1

```{r}
# load library
library(tidyposterior)
library(tidyverse)

# attach data
data("precise_example")

# look at data 
precise_example %>% head()

```

```{r}
# get classification accuracy results for each cross validated fold
accuracy <- precise_example %>% 
  select(id, contains("Accuracy")) %>% 
  setNames(tolower(gsub("_Accuracy$", "", names(.))))

accuracy
```

```{r}
# model accuracy results 
acc_model <- perf_mod(accuracy, seed = 13311, verbose = FALSE)

# extract posterior distributions 
accuracy_dists <- tidy(acc_model)

# credible intervals for accuracy per model 
summary(accuracy_dists)

```

### Plotting

```{r}
# create plot function
plot_model_posteriors <- function(data, model_chosen, geom_chosen = geom_point){
  data %>% 
    filter(model %in% model_chosen) %>% 
    ggplot(aes(x = model, y = posterior)) + 
    geom_chosen(alpha = 0.5, col = "#4B0082") + 
    scale_y_continuous(limits = c(0.4, 1))
}

# generate mapper
pmp <- as_mapper(partial(plot_model_posteriors, accuracy_dists))

# grab model list
model_list <- unique(accuracy_dists$model)

# map
posterior_plots <- map(model_list, pmp)

# create all together
all_together <- accuracy_dists %>% plot_model_posteriors(model_list)

gridExtra::grid.arrange(posterior_plots[[1]], posterior_plots[[2]], posterior_plots[[3]], all_together, nrow = 1)
```

# Example 2

The example that will be used is from the analysis of a fairly large classification data set using 10 fold cross validation with three models. 

```{r}
# check area under ROC curve
rocs <- precise_example %>% 
  select(id, contains("ROC")) %>% 
  set_names(tolower(gsub("_ROC$", "", names(.))))

rocs
```

```{r}
# gather ROCs for plotting comparison
rocs_stacked <- gather(rocs, key = model, value = statistic, -id)

# plot 
ggplot(rocs_stacked, aes(x = model, y = statistic, group = id, col = id)) + 
  geom_line(alpha = 0.75)
```

Since the lines are fairly parallel, there is likely to be a strong resample-to-resample effect. Note that the variation is fairly small; the within-model results don't vary a lot and are not near the ceiling of performance (i.e. an AUC of one). It also seems pretty clear that the models are producing different levels of performance. There also seems to be roughly equal variation for each model despite the difference in performance. 

## A Basic Linear Model 

When looking at resampling results, are the differences between models "real"? 

To answer this, a model can be created where the outcome is the resampling statistics (area under the ROC curve). These values are explained by the model types. In doing this, we can get the parameter estimates for each model's effect on the resampled ROC values and make statistical and practical comparisons between models. 

We will try a simple linear model with Gaussian errors that has a random effect for the resamples so that the within resample correlation can be estimated. Although the outcome is bounded in the interval [0, 1], the variability of these estimates might be precise enough to achieve a well fitting model. 

To fit the model, perf_mod will be used to fit a model using the stan_glmer function in the rstanarm package:

```{r}
# fit model
roc_model <- perf_mod(rocs, seed = 2824)
```

```{r}
# The `stan_glmer` model is contained in the element `roc_model$stan`
roc_model$stan
```

To ensure the vailidity of this fit, the shinystan package can be used to generate an interactive assessment of the model results. One other thing that we can do is to examine the posterior distributions to see if they make sense in terms of the range of values. 

## Getting the Posterior Distributions 

```{r}
# the tidy function can be used to extract the distributions into a simple data frame 
roc_post <- tidy(roc_model)

# plot 
ggplot(roc_post) + 
  # add the observed data to check for consistency
  geom_point(
    data = rocs_stacked, 
    aes(x = model, y = statistic), 
    alpha = 0.5, col = "#4B0082"
  )

```

These results look fairly reasonable given that we estimated a common variance for each of the models. 

## Comparing Models 

We will compare the generalized linear model with the neural network. Before doing so, it helps to specify what a real difference between models would be. Suppose that a 2% increase in accuracy was considered to be substantial. We can add this into the analysis. 

```{r}
# compute the posterior for the difference in RMSE for the two models 
glm_v_nnet <- contrast_models(roc_model, "nnet", "glm")

# look at data 
head(glm_v_nnet)
```

The summary function can be used to quantify this difference. It has an argument called size where we can add our belief about the size of a true difference.

```{r}
summary(glm_v_nnet, size = 0.02)
```

The probability column indicates the proportion of the posterior distribution that is greater than 0. This result indicates that the entire distribution is larger than one. The credible intervals reflect the large difference in the area under the ROC curves for these models.  The column pract_neg reflects the area where the posterior distribution is _less_ than -2% (i.e. practically negative). Similarly, the pract_pos column shows that most of the area is greater than 2% which leads us to believe that this is truly a substantial difference in performance. The pract_equiv reflects how much of the posterior is in [-2%, 2%]. If this were near one, it might indicate that the models are not practically different based on the yardstick of 2%. 

```{r}
# plot posterior of the differences
ggplot(glm_v_nnet, size = 0.02)
```

# Example 3 | Different Bayesian Models 

The dataset `noisy_example` contains the results of a series of regression models that were created from a small dataset with considerable variability. For resampling, 10 repeats of 10-fold cross validation were used to estimate performance. We will compare models using the root mean squared error. 

```{r}
# load data
data("noisy_example")

# look at data 
noisy_example %>% head()

# grab RMSEs
rmses <- noisy_example %>% 
  select(id, id2, contains("RMSE")) %>% 
  set_names(tolower(gsub("_RMSE$", "", names(.))))

rmses %>% head()

# gather RMSE for comparison
stacked_rmse <- gather(rmses, key = model, value = statistic, -c(id, id2))

# summarize 
mean_rmse <- stacked_rmse %>% 
  group_by(model) %>% 
  summarise(statistic = mean(statistic))

mean_rmse %>% head()

# plot RMSE across different runs
ggplot(stacked_rmse, aes(
  x = model, y = statistic, 
  group = paste(id, id2), 
  col = paste(id, id2))) + 
  geom_line(alpha = 0.75) + 
  theme(legend.position = "none")

# plot RMSE densities across runs
ggplot(stacked_rmse, aes(col = model, x = statistic)) + 
  geom_line(stat = "density", trim = FALSE) + 
  theme(legend.position = "top")
```

A few observations: 
  - The RMSE values vary 5-fold over the resampling results 
  - Many of the lines cross, indicating that the resample-to-resample variability might be larger than the model-to-model variability
  - The violin plots show right skewed distributions that, given the variability, are approaching the asymptote of 0.

### Transforming the Data 

Another approach is to transform the RMSE values to something model symmetric and model the data on a different scale. A log transform will be used here using the built-in object ln_trans. In using this option, the posterior distributions are computed on the log scale and is automatically back-transformed into the original units. By not passing family to the function, we are using a Gaussian model.

```{r}
log_linear_model <- perf_mod(rmses, transform = ln_trans, seed = 74)
```

```{r}
# look at posterior and means 
log_linear_post <- tidy(log_linear_model, seed = 3750)
log_linear_mean <- summary(log_linear_post)
log_linear_mean
```

```{r}
# plot 
ggplot(log_linear_post) + 
  geom_point(data = log_linear_mean, aes(y = mean), alpha = 0.5) + 
  geom_point(data = mean_rmse, aes(y = statistic), 
             col = "red", pch = 4, cex = 3)
```

The posteriors are a lot less skewed by the observed and estimated means are still fairly far away from one another. Since these differences are in the same direction, this would not appear to be related to the shrinkage properties of Bayesian models. 

## A Simple Gaussian Model 

```{r}
# fit a gaussian model for rmse estimates 
linear_model <- perf_mod(rmses, seed = 74)
```

```{r}
# get posterior and derive means 
linear_post <- tidy(linear_model, seed = 3750)
linear_mean <- summary(linear_post)

# plot
ggplot(linear_post) + 
  geom_point(data = linear_mean, aes(y = mean), alpha = 0.5) + 
  geom_point(data = mean_rmse, aes(y = statistic), 
             col = "red", pch = 4, cex = 3)

```

These are right on target. Despite the skewness of the original data, a simple linear model did best here. In hindsight, this makes sense since we are modeling _summary statistics_ as our outcome. Even if we believe these to be potentially skewed distributions, the central limit theorem is kicking in here and the estimates are trending to normality. 

We can compare models using the contrast_models function. The function has arguments for two sets of models to compare but if these are left to their default (NULL), all pair wise combinations are used. Let's say that an RMSE difference of 1 unit is important. 

```{r}
# get contrasts 
all_contrasts <- contrast_models(linear_model, seed = 8967)

# plot 
ggplot(all_contrasts, size = 1)

# get summary 
summary(all_contrasts, size = 1)
```

Based on our effect size of a single unit, the only pair that are practically equivalent are MARS and bagged trees. Since cubist has the smallest RMSE, it is not unreasonable to say that this model probides uniformly better results than the others shown here. 

## One Final Note 

The Bayesian models have population parameters for the model effects (akin to "fixed" effects in mixed models) as well as variance parameter(s) related to the resamples. The posteriors computed only reflect the mean parameters and should only be used to make inferences about this data set generally. This posterior calculation could not be used to predict the level of performance for a model a new _resample_ of the data. In this case, the variance paramaters come into play and the posterior would be much wider. In essence, the posteriors shown here are measuring the average performance value instead of a resample-specific value. 