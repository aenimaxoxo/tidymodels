---
title: "Tidy Models :: yardstick"
author: "Michael Rose"
output: 
  html_document:
     highlight: zenburn
     theme: lumen
     df_print: paged
     fig_align: center
---

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# {.tabset}
    
## Introduction

Yardstick is a package to estimate how well models are working using tidy data principles. This file is me working through tutorials found at [tidymodels :: yardstick](https://tidymodels.github.io/yardstick/index.html). 

These tutorials use yardstick version 0.0.3


## Load

```{r}
library(tidyverse)
library(tidymodels)
```

## Front Page


### Two Class Metric

Suppose we create a classification model and predict on a new data set. We may have data that looks like this: 

```{r}
two_class_example %>% head()
```

We can use dplyr like syntax to compute common performance characteristics of the model and get them back in the dataframe 

```{r}
metrics(two_class_example, truth, predicted)

# or

two_class_example %>%
    roc_auc(truth, Class1)
```

### Multiclass Metric

All classification metrics have at least one multiclass extension, with many of them having multiple ways to calculate multiclass metrics. 

```{r}
# load data
data("hpc_cv")

# place in tibble
(hpc_cv  <- as_tibble(hpc_cv))

# macro averaged multiclass precision
precision(hpc_cv, obs, pred)

# micro averaged multiclass precision
precision(hpc_cv, obs, pred, estimator = "micro")

```

### Calculating Metrics on Resamples

If we have multiple resamples of a model, we can use a metric on a grouped data frame to calculate the metric across all resamples at once. 

This calculates multiclass ROC AUC using the method described in Hand, Till (2001), and does it across all 10 resamples at once. 

> The area under the ROC curve, or the equivalent Gini index, is a widely used measure of performance of supervised classification rules. It has the attractive property that it side-steps the need to specify the costs of the different kinds of misclassification. However, the simple form is only applicable to the case of two classes. We extend the definition to the case of more than two classes by averaging pairwise comparisons. This measure reduces to the standard form in the two class case. We compare its properties with the standard measure of proportion correct and an alternative definition of proportion correct based on pairwise comparison of classes for a simple artificial case and illustrate its application on eight data sets. On the data sets we examined, the measures produced similar, but not identical results, reflecting the different aspects of performance that they were measuring. Like the area under the ROC curve, the measure we propose is useful in those many situations where it is impossible to give costs for the different kinds of misclassification.

Link : [A Simple Generalization of the Area Under the ROC Curve for Multiple Class Classification Problems](https://link.springer.com/content/pdf/10.1023%2FA%3A1010920819831.pdf)

```{r}
hpc_cv %>%
    group_by(Resample) %>%
    roc_auc(obs, VF:L)
```

### Autoplot Methods for Easy Visualization

```{r}
hpc_cv %>%
    group_by(Resample) %>%
    roc_curve(obs, VF:L) %>%
    autoplot()
```

### Quasiquotation

Quasiquotation can also be used to supply inputs 

```{r}
# probability columns
(lvl <- levels(two_class_example$truth))

two_class_example %>%
    mn_log_loss(truth, !!lvl[1])
```

## Metric Types 

There are three main metric types in `yardstick`: class, class probability, and numeric. 

1. Class Metrics (Hard Predictions)
   - truth : factor
   - estimate : factor
   
2. Class Probability Metrics (Soft Predictions)
   - truth : factor
   - estimate / ... : multiple numeric columns containing class probabilities
   
3. Numeric Metrics
   - truth : numeric
   - estimate : numeric 
   
### Example 

The `hpc_cv` data set contains class probabilities and class predictions for a linear discriminant analysis fit to the HPC data set of Kuhn and Johnson. It is fit with 10 fold cross validations, and the predictions for all folds are included. 

```{r}
hpc_cv %>%
    group_by(Resample) %>%
    slice(1:3)
```

1 metric, 1 resample 

```{r}
hpc_cv %>%
    filter(Resample == "Fold01") %>%
    accuracy(obs, pred)
```

1 metric, 10 resamples

```{r}
hpc_cv %>%
    group_by(Resample) %>%
    accuracy(obs, pred)
```

2 metrics, 10 resamples

```{r}
class_metrics <- metric_set(accuracy, kap)

hpc_cv %>%
    group_by(Resample) %>%
    class_metrics(obs, estimate = pred)

```

| type       | metric                 | Name                                     |
|------------|------------------------|------------------------------------------|
| class      | accuracy()             | Accuracy                                 |
|            | bal_accuracy()         | Balanced Accuracy                        |
|            | detection_prevalence() | Detection Prevalence                     |
|            | f_meas()               | F Measure                                |
|            | j_index()              | J Index                                  |
|            | kap()                  | Kappa                                    |
|            | mcc()                  | Matthews Correlation Coefficient         |
|            | npv()                  | Negative Predictive Value                |
|            | ppv()                  | Positive Predictive Value                |
|            | precision()            | Precision                                |
|            | recall()               | Recall                                   |
|            | sens()                 | Sensitivity                              |
|            | spec()                 | Specificity                              |
| class prob | gain_capture()         | Gain Capture                             |
|            | mn_log_loss()          | Mean Log Loss                            |
|            | pr_auc()               | Area Under the Precision Recall Curve    |
|            | roc_auc()              | Area Under Reciever Operator Curve       |
| numeric    | ccc()                  | Concordance Correlation Coefficient      |
|            | huber_loss()           | Huber Loss                               |
|            | huber_loss_pseudo()    | Pseudo Huber Loss                        |
|            | mae()                  | Mean Absolute Error                      |
|            | mape()                 | Mean Absolute Percent Error              |
|            | mase()                 | Mean Absolute Scaled Error               |
|            | rmse()                 | Root Mean Squared Error                  |
|            | rpd()                  | Ratio of Performance to Deviation        |
|            | rpiq()                 | Ratio of Performance to Inter Quartile   |
|            | rsq()                  | R Squared                                |
|            | rsq_trad()             | R Squared Traditional                    |
|            | smape()                | Symmetric Mean Absolute Percentage Error |
|------------|------------------------|------------------------------------------|


## Multiclass Averaging

Classification metrics in `yardstick` where both the truth and estimate columns are factors are implemented for the binary and multiclass case.

The multiclass implementations use `micro`, `macro`, and `macro_weighted` averaging where applicable, and some metrics have their own specialized multiclass implementations. 

### Macro Averaging

Macro averaging reduces your multiclass predictions down to multiple sets of binary predictions, calculates the corresponding metric for each of the binary cases, and then averages the results together. 

Consider precision for the binary case:

<center>
$Pr = \frac{TP}{TP + FP}$
</center>


In the multiclass case, if there were levels A, B, C, and D then macro averaging reduces the problem to multiple one vs all comparisons. The truth and estimate columns are recoded such that the only two levels are A and other, and then precision is calculated based on those recoded columns, with A being the relevant column. This process is then repeated for the other 3 levels to get a total of 4 precision values and the results are then averaged together. 

Note that in macro averaging, all classes get equal weight when contributing their portion of the precision value to the total. This may not be a realistic calculation when there exists large amounts of class imbalance. In this case, a weighted macro average might make more sense, where the weights are calculated by the frequency of that class in the truth column. 

<center>
$Pr_{weighted} = Pr_1 \frac{Obs_1}{N} + Pr_2 \frac{Obs_2}{N} + ... + Pr_n \frac{Obs_n}{N}$
</center>

### Micro Averaging

Micro averaging treats the entire set of data as an aggregate result, and calculates 1 metric rather than k metrics that get averaged together. For precision, this works by calculating all of the true positive results for each class and using that as the numerator, and then calculating all of the true positive and false positive results for each class and using that as the denominator. 

<center>
$Pr_{micro} = \frac{TP_1 + ... + TP_k}{(TP_1 + ... + TP_k) + (FP_1 + ... + FP_k)}$
</center>

In this case, rather than having each class getting equal weight, each observation gets equal weight. This gives the classes with the most observations more power.

### Specialized Multiclass Implementations

Some metrics have known analytical multiclass extensions, and do not need to use averaging to get an estimate of class performance. 

Accuracy and Kappa use the same definitions as their binary counterpart, with accuracy counting up the correctly predicted true values out of the number of true calues, and kappa being a linear combination of two accuracy values. 

Matthews Correlation Coefficient has a known multiclass generalization as well, sometimes called the $R_K$ statistic. Refer to [MCC Multiclass Case](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient#Multiclass_case)

ROC AUC is an interesting metric in that it intuitively makes sense to perform macro averaging, which computes a multiclass AUC as the average of the area under multiple binary ROC curves. However, this loses an important property of the ROC AUC statistic in that its binary case is insensitive to class distribution. To combat this, a multiclass metric was created that retains insensitivity to class distribution, but does not have an easy visual interpretation like macro averaging. This is implemented as the `hand-till` method, and is the default for this metric.

## Custom Metrics

`yardstick` includes a large number of metrics, but there is still the chance that there may be a custom metric that hasn't been implemented yet. 

We can create custom metrics with the infrastructure that yardstick provides: 

- Standardization between our metric and other preexisting metrics
- Automatic error handling for types and lengths
- Automatic selection of binary / multiclass metric implementations
- Automatic NA handling 
- Support for grouped data frames
- Support for use alongside other metrics in `metric_set()`

### Numeric Example - Mean Squared Error

Mean squared error is a numeric metric that measures the average of the squared errors. The formula for MSE is 

<center>
$MSE = \frac{1}{N} \sum\limits_{i = 1}^N (truth_i - estimate_i)^2 = mean((truth - estimate)^2)$
</center>

#### Vector Implementation

To create a vector function, we need to do two things: 

1. Create an internal implementation function, `mse_impl()`
2. Pass on that implementaion function to `metric_vec_template()`

`mse_impl()` contains the actual implementaion of the metric, and takes truth and estimate as arguments along with any metric specific arguments. 

`metric_vec_template()` is a yardstick function that accepts the implementation function along with other arguments to `mse_vec()` and actually executes `mse_impl()`. Additionally it has a `cls` argument to specify the allowed class type of truth and estimate. 

```{r}
library(yardstick)

# implement vector version of the MSE metric
mse_vec <- function(truth, estimate, na_rm = TRUE, ...) {
    mse_impl <- function(truth, estimate) {
        mean((truth - estimate)^2)
    }

    metric_vec_template(
        metric_impl = mse_impl,
        truth = truth,
        estimate = estimate,
        na_rm = na_rm,
        cls = "numeric",
        ...
    )
}
```

Let's try it out

```{r}
# load data
data("solubility_test")

mse_vec(
    truth = solubility_test$solubility,
    estimate = solubility_test$prediction
)

```

Intelligent error handling is immediately avaible

```{r}
# mse_vec(truth = "apple", estimate = 1)
# mse_vec(truth = 1, estimate = factor("xyz"))
```

NA values are removed if `na_rm = TRUE`. If `na_rm = FALSE` and any NA values are detected, then the metric automatically returns NA. 

```{r}
# na values removed
mse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5))

# na returned
mse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5), na_rm = FALSE)
```

#### Data Frame Implementation

The data frame version of the metric is a generic function with a data.frame method that calls the yardstick helper `metric_summarizer()` and passes the `mse_vec()` function to it along with truth and estimate that have been wrapped in `rlang::enquo()` and then unquoted with `!!` so that non standard evaluation can be supported.

```{r}
library(rlang)

mse <- function(data, ...) {
    UseMethod("mse")
}

mse.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
    metric_summarizer(
        metric_nm = "mse",
        metric_fn = mse_vec,
        data = data,
        truth = !! enquo(truth),
        estimate = !! enquo(estimate),
        na_rm = na_rm,
        ...
    )
}

```

And that's it. `yardstick` handles the rest with an internal call to `summarise()`. 

```{r}
mse(solubility_test, truth = solubility, estimate = prediction)

# error handling
# mse(solubility_test, truth = solubility, estimate = factor("xyz"))
```

Let's test it out on a grouped data frame

```{r}
set.seed(8888)
size <- 100
times <- 10

# create 10 resamples
solubility_resampled <- bind_rows(
    replicate(
        n = times,
        expr = sample_n(solubility_test, size, replace = TRUE),
        simplify = FALSE
    ),
    .id = "resample"
)

solubility_resampled %>%
    group_by(resample) %>%
    mse(solubility, prediction)
```

### Class Example - Miss Rate

Miss rate is another name for the False Negative Rate, and is a classification metric in the same family as sens() and spec(). It follows the formula 

<center>
$miss_rate = \frac{FN}{FN + TP}$
</center>

This metric, like other classification metrics, is more easily computed when expressed as a confusion matrix. Classification metrics are more complicated than numeric ones because we have to think about extensions for the multiclass case as well as the binary case. 

#### Vector Implementation

```{r}
# so we can support the yardstick event_first option
relevant_col <- function(xtab) {
    if (getOption("yardstick.event_first")) {
        colnames(xtab)[1]
    } else {
        colnames(xtab)[2]
    }
}

miss_rate_vec <- function(truth, estimate, estimator = NULL, na_rm = TRUE, ...) {
    estimator <- finalize_estimator(truth, estimator)

    miss_rate_impl <- function(truth, estimate) {
        # create
        xtab <- table(estimate, truth)
        col <- relevant_col(xtab)
        col2 <- setdiff(colnames(xtab), col)

        tp <- xtab[col, col]
        fn <- xtab[col2, col]

        fn / (fn + tp)
    }

    metric_vec_template(
        metric_impl = miss_rate_impl,
        truth = truth,
        estimate = estimate,
        na_rm = na_rm,
        cls = "factor",
        estimator = estimator,
        ...
    )
}

```

Another change from the numeric metric is that a call to `finalize_estimator()` is made. This is the infrastructure that auto-selects the type of estimator to use. 

```{r}
data("two_class_example")

miss_rate_vec(two_class_example$truth, two_class_example$predicted)
```

What happens if we try to pass in a multiclass result? 

```{r}
data("hpc_cv")

fold1 <- hpc_cv %>% filter(Resample == "Fold01")

miss_rate_vec(fold1$obs, fold1$pred)
```

This isn't ideal, as currently multiclass `miss_rate()` isn't supported and it would have been better to throw an error if the estimator wasn't binary. 
 
To fix this, a generic counterpart to `finalize_estimator()` called `finalize_estimator_internal()` exists that helps us restrict the input types. It is also good practice to call `validate_estimator()` which handles the case where a user passed in the estimator themselves. 

```{r}
finalize_estimator_internal.miss_rate <- function(metric_dispatcher, x, estimator) {
    validate_estimator(estimator, estimator_override = "binary")

    if (!is.null(estimator)) {
        return(estimator)
    }

    lvls <- levels(x)

    if (length(lvls) > 2) {
        stop("A multiclass `truth` input was provided, but only `binary` is supported.")
    }
    "binary"
}

miss_rate_vec <- function(truth, estimate, estimator = NULL, na_rm = TRUE, ...) {
    # calls finalize_estimator_internal()
    estimator <- finalize_estimator(truth, estimator, metric_class = "miss_rate")

    miss_rate_impl <- function(truth, estimate) {
        # create
        xtab <- table(estimate, truth)
        col <- relevant_col(xtab)
        col2 <- setdiff(colnames(xtab), col)
        tp <- xtab[col, col]
        fn <- xtab[col2, col]

        fn / (fn + tp)
    }

    metric_vec_template(
        metric_impl = miss_rate_impl,
        truth = truth,
        estimate = estimate,
        na_rm = na_rm,
        cls = "factor",
        estimator = estimator,
        ...
    )
} 

```

Let's try it out 

```{r}
# error thrown by custom handler
# miss_rate_vec(fold1$obs, fold1$pred)

# error thrown by validate_estimator
# miss_rate_vec(fold1$obs, fold1$pred, estimator = "macro")
```

#### Supporting Multiclass Miss Rate

Like many other classification metrics such as precision or recall, miss rate doesnt have a natural multiclass extension, but one can be created using methods such as macro, weighted macro, and micro averaging. Generally they require more effort to get right than the binary case, especially if we wish to have a performant version. Luckily a somewhat standard template is used in yardstick. 

```{r}
# remove the binary restriction we wrote earlier
rm(finalize_estimator_internal.miss_rate)
```

The main changes below are: 

- The binary implementation is moved to `miss_rate_binary()`

- `miss_rate_estimator_impl()` is a helper function for switching between binary and multiclass implementations. It also applies the weighting required for multiclass estimators. 

- `miss_rate_multiclass()` provides the implementation for the multiclass case. It calculates the true positive and false negative values as vectors with one value per class. For the macro case, it returns a vector of miss rate calculations, and for micro it first sums the individual pieces and returns a single miss rate calculation. In the macro case, the vector is then weighted approriately in `miss_rate_estimator_impl()` depending on whether or not it was macro or weighted macro. 

```{r}
miss_rate_vec <- function(truth, estimate, estimator = NULL, na_rm = TRUE, ...) {
    # calls finalize_estimator_internal() internally
    estimator <- finalize_estimator(truth, estimator, metric_class = "miss_rate")

    miss_rate_impl <- function(truth, estimate) {
        xtab <- table(estimate, truth)

        # rather than implement the method here, we rely on an *_estimator() function that can handle binary and multiclass cases
        miss_rate_estimator_impl(xtab, estimator)
    }

    metric_vec_template(
        metric_impl = miss_rate_impl,
        truth = truth,
        estimate = estimate,
        na_rm = na_rm,
        cls = "factor",
        estimator = estimator,
        ...
    )
}

# This function switches between binary and multiclass implementations
miss_rate_estimator_impl <- function(data, estimator) {
    if (estimator == "binary") {
        miss_rate_binary(data)
    } else {
        # excapsulte the macro, macro weighted and micro cases
        wt <- get_weights(data, estimator)
        res <- miss_rate_multiclass(data, estimator)
        weighted.mean(res, wt)
    }
}

miss_rate_binary <- function(data) {
    col <- relevant_col(data)
    col2 <- setdiff(colnames(data), col)
    tp <- data[col, col]
    fn <- data[col2, col]

    fn / (fn + tp) 
}

miss_rate_multiclass <- function(data, estimator) {
    # we need tp and fn for all classes individually. tp + fn = colSums(data)
    tp <- diag(data)
    tpfn <- colSums(data)
    fn <- tpfn - tp

    # if using a micro estimator, we sum the individual pieces before performing miss rate calc
    if (estimator == "micro") {
        tp <- sum(tp)
        fn <- sum(fn)
    }

    # return the vector
    tp / (tp + fn)
}

```

For the macro case, this separation of weighting from the core implementation might seem strange, but there is a good reason for it. Some metrics are combinations of other metrics, and it is nice to be able to reuse code when calculating more complex metrics. For example, `f_meas()` is a combination of `recall` and `precision`. 

Let's try it out now

```{r}
# two class
miss_rate_vec(two_class_example$truth, two_class_example$predicted)

# multiclass
miss_rate_vec(fold1$obs, fold1$pred)
```

### Data Frame Implementation

Luckily, theh data frame implementation is simple (much like in the numeric case). 

```{r}
miss_rate <- function(data, truth, estimate, estimator = NULL, na_rm = TRUE, ...) {
    UseMethod("miss_rate")
}

miss_rate.data.frame <- function(data, truth, estimate, estimator = NULL, na_rm = TRUE, ...){
    metric_summarizer(
        metric_nm = "miss_rate",
        metric_fn = miss_rate_vec,
        data = data,
        truth = !! enquo(truth),
        estimate = !! enquo(estimate),
        estimator = estimator,
        na_rm = na_rm,
        ...
    )
}

```

Let's try it out

```{r}
# macro weighted automatically selected
fold1 %>%
    miss_rate(obs, pred)

# micro weighted
fold1 %>%
    miss_rate(obs, pred, estimator = "micro")

# macro weighted by resample
hpc_cv %>%
    group_by(Resample) %>%
    miss_rate(obs, pred, estimator = "macro_weighted")
```

Lets also check error handling

```{r}
# miss_rate(hpc_cv, obs, VF)
```

#### Using Custom Metrics in metric_set()

`metric_set()` validates that all metric functions are of the same metric type by checking the class of the function. If any metrics are not of the right class, `metric_set()` fails. This means that to use your function with `metric_set()`, you need to add the correct class. 

- numeric metrics - "numeric_metric"
- class metrics - "class_metric"
- class probability metrics - "prob_metric"

```{r}
# this errors because the class hasn't been set
# metric_set(mse, rmse)
```

> Error: The combination of metric functions must be:
> - only numeric metrics
> - a mix of class metrics and class probability metrics
> The following metric function types are being mixed:
> - other (mse)
> - numeric (rmse)
